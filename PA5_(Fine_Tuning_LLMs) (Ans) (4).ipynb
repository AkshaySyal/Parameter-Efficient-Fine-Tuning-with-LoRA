{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efc30363",
      "metadata": {
        "id": "efc30363"
      },
      "source": [
        "# Fine-Tuning LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78bbda16",
      "metadata": {
        "id": "78bbda16"
      },
      "source": [
        "In this exercise, you will fine-tune the [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model for enhanced dialogue summarization. You will first explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter-Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae38df4f",
      "metadata": {
        "id": "ae38df4f"
      },
      "source": [
        "## 1. Set up Dependencies and Load Dataset and LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cfaa9a71",
      "metadata": {
        "id": "cfaa9a71"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate rouge_score peft -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7bfc5e61",
      "metadata": {
        "id": "7bfc5e61"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e0022db",
      "metadata": {
        "id": "9e0022db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db20180-636b-4325-93e9-5ff0cd8aea1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('knkarthick/dialogsum')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFoeF8p3VDKg",
        "outputId": "2e6b1d9f-86b1-434d-96ab-a835dab59df3"
      },
      "id": "sFoeF8p3VDKg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829a483e",
      "metadata": {
        "id": "829a483e"
      },
      "source": [
        "Load the pre-trained [Flan-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of Flan-T5. Setting `torch_dtype=torch.bfloat16` specifies the data type to be used by this model, which can reduce GPU memory usage since `bfloat16` uses half as much memory per number compared to `float32`, the default precision for most models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3b475615",
      "metadata": {
        "id": "3b475615"
      },
      "outputs": [],
      "source": [
        "model_name = 'google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404331c9",
      "metadata": {
        "id": "404331c9"
      },
      "source": [
        "## 2. Test the Model with Zero-Shot Inferencing\n",
        "\n",
        "Test the model with zero-shot inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "903afec6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903afec6",
        "outputId": "83505cb4-647d-4e50-830c-5de9234695c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n",
            "#Person2#: You look a bit pale, don't you?\n",
            "#Person1#: Yes, I can't sleep well every night.\n",
            "#Person2#: You should get plenty of sleep.\n",
            "#Person1#: I drink a lot of wine.\n",
            "#Person2#: If I were you, I wouldn't drink too much.\n",
            "#Person1#: I often feel so tired.\n",
            "#Person2#: You better do some exercise every morning.\n",
            "#Person1#: I sometimes find the shadow of death in front of me.\n",
            "#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION:\n",
            "Person1: I'm not sure how to adjust my life.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "index = 42\n",
        "dash_line = '-' * 100\n",
        "\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION:\\n{original_model_summary}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hqKrbmjxvXhn",
      "metadata": {
        "id": "hqKrbmjxvXhn"
      },
      "source": [
        "You can see that the model struggles to summarize the dialogue compared to the baseline summary, and simply repeats the first sentence from the dialogue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39269c1c",
      "metadata": {
        "id": "39269c1c"
      },
      "source": [
        "## 3. Perform Full Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb795b4",
      "metadata": {
        "id": "ebb795b4"
      },
      "source": [
        "### 3.1 Preprocess the Dataset\n",
        "\n",
        "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation.`, and to the start of the summary with `Summary:` as follows:\n",
        "\n",
        "Training prompt (dialogue):\n",
        "```\n",
        "Summarize the following conversation.\n",
        "Alice: This is her part of the conversation.\n",
        "Bob: This is his part of the conversation.    \n",
        "Summary:\n",
        "```\n",
        "\n",
        "Training response (summary):\n",
        "```\n",
        "Both Alice and Bob participated in the conversation.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EVGHecAnv8s0",
      "metadata": {
        "id": "EVGHecAnv8s0"
      },
      "source": [
        "**Exercise**: Write a function to tokenize a batch of examples from the dialogue dataset. The function should concatentate the dialogues with the predefined prompt, tokenize them along with their summaries, and define the tokenized summaries as the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "T6bMPv3ZK3UD",
      "metadata": {
        "id": "T6bMPv3ZK3UD"
      },
      "outputs": [],
      "source": [
        "def tokenize(examples):\n",
        "    ### WRITE YOUR CODE HERE\n",
        "    prefix = \"Summarize the following conversation.\\n\"\n",
        "    postfix = \"\\nSummary:\"\n",
        "    inputs = [prefix + doc + postfix for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "JPtVA3XzK5OG",
      "metadata": {
        "id": "JPtVA3XzK5OG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a2ca13247ecb442997141a3e0397ad91",
            "f5c21d60eda54e7f8a73013f075c28e9",
            "f21e9b3fa78f4c36b9a331c2dd29bd49",
            "1881a5002a7147ae8c5af7e8b8a45209",
            "54a154d2f58a4015b3a309acc74a90b9",
            "7b6e3f08b6ef4bd88b486eeecc02a2cf",
            "a98d2900cda9427b83c9f428db11e6c2",
            "fbe892584a2a4b67b150f06f527a8655",
            "4a877ea25c334777a6a6af952cf14640",
            "eb25c4ac19d644528a5e7383b140538a",
            "06525d92e5944f7ea53568e754e6b76a"
          ]
        },
        "outputId": "72a05763-6948-4030-aea3-9a45813bcf26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2ca13247ecb442997141a3e0397ad91"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101bb5da",
      "metadata": {
        "id": "101bb5da"
      },
      "source": [
        "### 3.2 Fine-Tune the Model\n",
        "\n",
        "**Exercise**: Utilize the Hugging Face Trainer API for training the model on the preprocessed dataset. Define the training arguments, a data collator, and create a `Seq2SeqTrainer` instance. Train the model for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training args\n",
        "batch_size = 16\n",
        "num_train_epochs = 1\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/PA5 Assignment\",\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=16,\n",
        "    eval_accumulation_steps=16,\n",
        "    weight_decay=0.01,\n",
        "    log_level=\"debug\",\n",
        "    logging_strategy=\"steps\",\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    seed=1,\n",
        "    bf16=True,\n",
        "    bf16_full_eval=True,\n",
        "    eval_steps=10,\n",
        "    logging_steps=10,\n",
        "    save_strategy='steps',\n",
        "    save_steps = 0.2,\n",
        "    save_total_limit=3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-VCB_xnpThqL"
      },
      "id": "-VCB_xnpThqL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)"
      ],
      "metadata": {
        "id": "KxYYKy3aTkK-"
      },
      "id": "KxYYKy3aTkK-",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2SeqTrainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=original_model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "8f5MYUlxTmzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcf40db-938b-494e-c84d-6477373c750c"
      },
      "id": "8f5MYUlxTmzF",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Using auto half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070b86e4",
      "metadata": {
        "id": "070b86e4"
      },
      "source": [
        "Training a fully fine-tuned version of the model should take about 10 minutes on a Google Colab GPU machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "22e44303",
      "metadata": {
        "id": "22e44303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1bbc21a-4037-4e0a-e0e5-3d7267b800bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Currently training with a batch size of: 16\n",
            "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 12,460\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 48\n",
            "  Number of trainable parameters = 247,577,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 02:59, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.559600</td>\n",
              "      <td>1.171397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.270900</td>\n",
              "      <td>1.111279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.221300</td>\n",
              "      <td>1.082012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.199900</td>\n",
              "      <td>1.070874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10/config.json\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-10/spiece.model\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20/config.json\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-20/spiece.model\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30/config.json\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-30/spiece.model\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40/config.json\n",
            "Configuration saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/tmp-checkpoint-40/spiece.model\n",
            "Deleting older checkpoint [/content/drive/MyDrive/PA5 Assignment/checkpoint-10] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=48, training_loss=1.2919357220331829, metrics={'train_runtime': 184.2385, 'train_samples_per_second': 67.63, 'train_steps_per_second': 0.261, 'total_flos': 7828445634527232.0, 'train_loss': 1.2919357220331829, 'epoch': 0.99})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcOg9qQ_9M5M",
      "metadata": {
        "id": "bcOg9qQ_9M5M"
      },
      "source": [
        "Save the model to a local folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "klGQxAQf7prf",
      "metadata": {
        "id": "klGQxAQf7prf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983b828d-e90f-4dc0-9ceb-a5e9f0e4e3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./flan-t5-base-dialogsum-checkpoint/config.json\n",
            "Configuration saved in ./flan-t5-base-dialogsum-checkpoint/generation_config.json\n",
            "Model weights saved in ./flan-t5-base-dialogsum-checkpoint/model.safetensors\n",
            "tokenizer config file saved in ./flan-t5-base-dialogsum-checkpoint/tokenizer_config.json\n",
            "Special tokens file saved in ./flan-t5-base-dialogsum-checkpoint/special_tokens_map.json\n",
            "Copy vocab file to ./flan-t5-base-dialogsum-checkpoint/spiece.model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./flan-t5-base-dialogsum-checkpoint/tokenizer_config.json',\n",
              " './flan-t5-base-dialogsum-checkpoint/special_tokens_map.json',\n",
              " './flan-t5-base-dialogsum-checkpoint/spiece.model',\n",
              " './flan-t5-base-dialogsum-checkpoint/added_tokens.json',\n",
              " './flan-t5-base-dialogsum-checkpoint/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model_path = './flan-t5-base-dialogsum-checkpoint'\n",
        "#model_path = '/content/drive/MyDrive/PA5 (Fine-Tuning LLMs) 1/flan-t5-base-dialogsum-checkpoint-1'\n",
        "\n",
        "original_model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814042dd",
      "metadata": {
        "id": "814042dd"
      },
      "source": [
        "Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d98f4126",
      "metadata": {
        "id": "d98f4126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3d4b45-d0db-485f-ba8a-eb11d951d4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./flan-t5-base-dialogsum-checkpoint/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"./flan-t5-base-dialogsum-checkpoint\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file ./flan-t5-base-dialogsum-checkpoint/model.safetensors\n",
            "Instantiating T5ForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./flan-t5-base-dialogsum-checkpoint.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file ./flan-t5-base-dialogsum-checkpoint/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-base-dialogsum-checkpoint',\n",
        "                                                       torch_dtype=torch.bfloat16)\n",
        "# instruct_model = AutoModelForSeq2SeqLM.from_pretrained('/content/drive/MyDrive/PA5 (Fine-Tuning LLMs)/checkpoint-80',\n",
        "#                                                        torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dK1EsTihBrHJ",
      "metadata": {
        "id": "dK1EsTihBrHJ"
      },
      "source": [
        "Reload the original Flan-T5-base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "mdS7JS6PBpAh",
      "metadata": {
        "id": "mdS7JS6PBpAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13121afa-205a-43e5-cbb8-7f0d321ab057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n",
            "Instantiating T5ForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "964bc7e3",
      "metadata": {
        "id": "964bc7e3"
      },
      "source": [
        "### 3.3 Evaluate the Model Qualitatively (Human Evaluation)\n",
        "\n",
        "**Exercise**: Make inferences for the same example as in Section 2, using the original model and the fully fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e10df481",
      "metadata": {
        "id": "e10df481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3f118e-0e55-42c8-a160-1ddf9db6eefd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n",
            "#Person2#: You look a bit pale, don't you?\n",
            "#Person1#: Yes, I can't sleep well every night.\n",
            "#Person2#: You should get plenty of sleep.\n",
            "#Person1#: I drink a lot of wine.\n",
            "#Person2#: If I were you, I wouldn't drink too much.\n",
            "#Person1#: I often feel so tired.\n",
            "#Person2#: You better do some exercise every morning.\n",
            "#Person1#: I sometimes find the shadow of death in front of me.\n",
            "#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION:\n",
            "Person1: I'm not sure how to adjust my life.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "INSTRUCT MODEL GENERATION:\n",
            "#Person1# can't sleep well every night and drinks a lot of wine. #Person2# advises #Person1# to do some exercise every morning.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### WRITE YOUR CODE HERE\n",
        "index = 42\n",
        "dash_line = '-' * 100\n",
        "\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "original_model_output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "original_model_summary = tokenizer.decode(original_model_output, skip_special_tokens=True)\n",
        "\n",
        "instruct_model_output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "instruct_model_summary = tokenizer.decode(instruct_model_output, skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION:\\n{original_model_summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL GENERATION:\\n{instruct_model_summary}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c65474b3",
      "metadata": {
        "id": "c65474b3"
      },
      "source": [
        "The fine-tuned model is able to create a much better summary of the dialogue compared to the original model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1ffbd2",
      "metadata": {
        "id": "4b1ffbd2"
      },
      "source": [
        "### 3.4 Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "\n",
        "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cf2a480b",
      "metadata": {
        "id": "cf2a480b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9dc82e1aa5eb4b80803a3abc9e505b9d",
            "a44358bf333c42968479c8415eb46ff9",
            "7b99c3c3b2264a82a3004aca89d33df3",
            "ae37ceb237f041c4aef0a876d2fdd226",
            "a87ee9ebfd78459784060a4869af6e02",
            "623d2701f31c4f22b36a3222373e9c80",
            "e11c2c3d964d4b64b1084180ea76e83c",
            "ea6aa9c2987d4e818f75fe817782dc3a",
            "d81400fd9e3d4c77aafe129a948abbcf",
            "165719c3511b44939b6368c6944746a3",
            "7326b9e8db6647c19837fa7bb69c38fc"
          ]
        },
        "outputId": "5ee49090-8746-4057-fa41-a9a8fdfe5316"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dc82e1aa5eb4b80803a3abc9e505b9d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "in2SYXRfCNWA",
      "metadata": {
        "id": "in2SYXRfCNWA"
      },
      "source": [
        "**Exercise**: Generate the outputs for a sample of the test set with the fine-tuned model (use only the first 10 dialogues and summaries to save time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "pAOnB6lFCOUw",
      "metadata": {
        "id": "pAOnB6lFCOUw"
      },
      "outputs": [],
      "source": [
        "### WRITE YOUR CODE HERE\n",
        "human_baseline_summaries = []\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "\n",
        "for idx in range(10):\n",
        "  dialogue = dataset['test'][idx]['dialogue']\n",
        "  summary = dataset['test'][idx]['summary']\n",
        "  human_baseline_summaries.append(summary)\n",
        "\n",
        "  prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "  original_model_output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "  original_model_summary = tokenizer.decode(original_model_output, skip_special_tokens=True)\n",
        "\n",
        "  instruct_model_output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "  instruct_model_summary = tokenizer.decode(instruct_model_output, skip_special_tokens=True)\n",
        "\n",
        "  original_model_summaries.append(original_model_summary)\n",
        "  instruct_model_summaries.append(instruct_model_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a019aaa",
      "metadata": {
        "id": "4a019aaa"
      },
      "source": [
        "Evaluate the models computing ROUGE metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d77847c0",
      "metadata": {
        "id": "d77847c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124ef7b3-e3ae-477c-d5d5-5f7aa3464489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.22754314914198637, 'rouge2': 0.09982267689684571, 'rougeL': 0.2116144901610018, 'rougeLsum': 0.21135161167428607}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': 0.30749690080369374, 'rouge2': 0.07752467850387067, 'rougeL': 0.23535596814257592, 'rougeLsum': 0.23560446397803886}\n"
          ]
        }
      ],
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)]\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)]\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c44701b",
      "metadata": {
        "id": "1c44701b"
      },
      "source": [
        "The results show substantial improvement in all ROUGE metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e2d1a51a",
      "metadata": {
        "id": "e2d1a51a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a8327c-1e4d-4d8b-a25f-d163b6b39e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute percentage improvement of the instruct model over the original model:\n",
            "rouge1: 8.00%\n",
            "rouge2: -2.23%\n",
            "rougeL: 2.37%\n",
            "rougeLsum: 2.43%\n"
          ]
        }
      ],
      "source": [
        "print(\"Absolute percentage improvement of the instruct model over the original model:\")\n",
        "\n",
        "for key in instruct_model_results:\n",
        "    improvement = instruct_model_results[key] - original_model_results[key]\n",
        "    print(f'{key}: {improvement*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iq72DeUafKOL",
      "metadata": {
        "id": "Iq72DeUafKOL"
      },
      "source": [
        "## 4. Perform Parameter Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** instead of \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning, with comparable evaluation results as you will see soon.\n",
        "\n",
        "One of the most popular PEFT methods is **Low-Rank Adaptation (LoRA)**, which  introduces low-rank matrices to adapt the LLM with minimal additional parameters. In most cases, when someone says PEFT, they typically mean LoRA.  After fine-tuning for a specific task with LoRA, the result is that the original LLM remains unchanged and a newly-trained \"LoRA adapter\" emerges. This LoRA adapter is much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
        "\n",
        "At inference time, the LoRA adapter is reunited and combined with its original LLM to serve the inference request. The benefit is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jjMz_LZrfRKN",
      "metadata": {
        "id": "jjMz_LZrfRKN"
      },
      "source": [
        "### 4.1 Setup the LoRA model for Fine-Tuning\n",
        "\n",
        "You first need to define the configuration of the LoRA model. Have a look at the configuration below. The key configuration element to adjust is the rank (`r`) of the adapter, which influences its capacity and complexity. Experiment with various ranks, such as 8, 16, or 32, and see how they affect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c52eb8bf",
      "metadata": {
        "id": "c52eb8bf"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lrxAQD2tflZA",
      "metadata": {
        "id": "lrxAQD2tflZA"
      },
      "source": [
        "Add LoRA adapter layers/parameters to the original LLM to be trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1IHrKzPnfL-n",
      "metadata": {
        "id": "1IHrKzPnfL-n"
      },
      "outputs": [],
      "source": [
        "peft_model = get_peft_model(original_model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b4ae88c",
      "metadata": {
        "id": "3b4ae88c"
      },
      "source": [
        "The number of trainable model parameters in the LoRA model is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e35a2022",
      "metadata": {
        "id": "e35a2022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44d6e39-6062-438a-abec-9852123d3f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,538,944 || all params: 251,116,800 || trainable%: 1.4092820552029972\n"
          ]
        }
      ],
      "source": [
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zpLzokfYfo_m",
      "metadata": {
        "id": "zpLzokfYfo_m"
      },
      "source": [
        "### 4.2 Train the LoRA Adapter\n",
        "\n",
        "**Exercise**: Define training arguments and create a `Seq2SeqTrainer` instance for the LoRA model. Use a higher learning rate than full fine-tuning (e.g., `1e-3`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5oEIIiIofrC8",
      "metadata": {
        "id": "5oEIIiIofrC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f36ad9-03cb-46dd-e685-7b48dd731530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "### WRITE YOUR CODE HERE\n",
        "\n",
        "peft_training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/PA5 Assignment/PEFT\",\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=16,\n",
        "    eval_accumulation_steps=16,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    logging_strategy=\"steps\",\n",
        "    seed=42,\n",
        "    bf16=True,\n",
        "    bf16_full_eval=True,\n",
        "    eval_steps=10,\n",
        "    save_strategy='steps',\n",
        "    save_steps = 0.2,\n",
        "    logging_steps = 10,\n",
        "    save_total_limit=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.config.use_cache = False\n",
        "\n",
        "peft_trainer = Seq2SeqTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    args=peft_training_args,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhOBrhvpjToA",
        "outputId": "ab0a9aed-3a10-4840-97b5-2b1ca349f633"
      },
      "id": "fhOBrhvpjToA",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Using auto half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_2jryxZgMdR",
      "metadata": {
        "id": "H_2jryxZgMdR"
      },
      "source": [
        "Train the PEFT adapter. Training should take about 6 minutes on a Google Colab GPU machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f0d7T_P1gNlP",
      "metadata": {
        "id": "f0d7T_P1gNlP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4794758-0144-4946-bde4-63b5c8264378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Currently training with a batch size of: 16\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 12,460\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 48\n",
            "  Number of trainable parameters = 3,538,944\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 02:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.604200</td>\n",
              "      <td>1.239341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.358600</td>\n",
              "      <td>1.172541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.306200</td>\n",
              "      <td>1.148917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.275700</td>\n",
              "      <td>1.141850</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-10\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-10/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-10/spiece.model\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-20\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-20/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-20/spiece.model\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-30\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-30/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-30/spiece.model\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: id, summary, dialogue, topic. If id, summary, dialogue, topic are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-40\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-40/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/PA5 Assignment/PEFT/tmp-checkpoint-40/spiece.model\n",
            "Deleting older checkpoint [/content/drive/MyDrive/PA5 Assignment/PEFT/checkpoint-10] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=48, training_loss=1.3687174320220947, metrics={'train_runtime': 181.1582, 'train_samples_per_second': 68.78, 'train_steps_per_second': 0.265, 'total_flos': 8053188106862592.0, 'train_loss': 1.3687174320220947, 'epoch': 0.99})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "peft_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vJ_h6KU2gcUf",
      "metadata": {
        "id": "vJ_h6KU2gcUf"
      },
      "source": [
        "Save the model to a local folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7T9fwZ0NhOKC",
      "metadata": {
        "id": "7T9fwZ0NhOKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de935540-603b-4af0-e7bd-ee1b6ccdf1a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peft_model.save_pretrained('./flan-t5-base-dialogsum-lora')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmqvrFOrhVby",
      "metadata": {
        "id": "KmqvrFOrhVby"
      },
      "source": [
        "Load the PEFT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Wit_mz3Vgh-V",
      "metadata": {
        "id": "Wit_mz3Vgh-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd7e95f-4a1f-41bb-ac97-71024fd4593c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/spiece.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "from peft import AutoPeftModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "peft_model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-base-dialogsum-lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dogDPmBBkuya",
      "metadata": {
        "id": "dogDPmBBkuya"
      },
      "source": [
        "Reload the original Flan-T5-base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "EFur6v7nkItQ",
      "metadata": {
        "id": "EFur6v7nkItQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0bdc8f-a170-47eb-ab95-6ebc0f28b328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n",
            "Instantiating T5ForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-AHhrS2lheOH",
      "metadata": {
        "id": "-AHhrS2lheOH"
      },
      "source": [
        "### 4.3 Evaluate the Model Qualitatively (Human Evaluation)\n",
        "\n",
        "**Exercise**: Make inferences for the same example as in Sections 2 and 3, using the original model, the fully fine-tuned model and the PEFT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "yGFEKSwAhXr6",
      "metadata": {
        "id": "yGFEKSwAhXr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1978a319-435f-4c19-f139-3bb59a7e8e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "DIALOGUE:\n",
            "#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n",
            "#Person2#: You look a bit pale, don't you?\n",
            "#Person1#: Yes, I can't sleep well every night.\n",
            "#Person2#: You should get plenty of sleep.\n",
            "#Person1#: I drink a lot of wine.\n",
            "#Person2#: If I were you, I wouldn't drink too much.\n",
            "#Person1#: I often feel so tired.\n",
            "#Person2#: You better do some exercise every morning.\n",
            "#Person1#: I sometimes find the shadow of death in front of me.\n",
            "#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL GENERATION - ZERO SHOT:\n",
            "Person1: I'm not sure how to adjust my life.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "FINETUNED MODEL GENERATION:\n",
            "#Person1# can't sleep well every night and drinks a lot of wine. #Person2# advises #Person1# to do some exercise every morning.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "PEFT MODEL GENERATION:\n",
            "#Person1# is pale and doesn't know how to adjust his life. #Person2# advises #Person1# to get plenty of sleep and exercise.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "index = 42\n",
        "dash_line = '-' * 100\n",
        "\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "fine_tuned_output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "fine_tuned_model_summary = tokenizer.decode(fine_tuned_output, skip_special_tokens=True)\n",
        "\n",
        "peft_output = peft_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "peft_model_summary = tokenizer.decode(peft_output, skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'DIALOGUE:\\n{dialogue}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL GENERATION - ZERO SHOT:\\n{original_model_summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'FINETUNED MODEL GENERATION:\\n{fine_tuned_model_summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL GENERATION:\\n{peft_model_summary}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YzNd3ptflWeS",
      "metadata": {
        "id": "YzNd3ptflWeS"
      },
      "source": [
        "### 4.4 Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "\n",
        "**Exercise**: Generate the outputs for a sample of the test set with the PEFT model (use only the first 10 dialogues and summaries to save time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "lkDE0mtrkD1K",
      "metadata": {
        "id": "lkDE0mtrkD1K"
      },
      "outputs": [],
      "source": [
        "### WRITE YOUR CODE HERE\n",
        "human_baseline_summaries = []\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "for idx in range(10):\n",
        "  dialogue = dataset['test'][idx]['dialogue']\n",
        "  summary = dataset['test'][idx]['summary']\n",
        "  human_baseline_summaries.append(summary)\n",
        "\n",
        "  prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "  original_model_output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "  original_model_summary = tokenizer.decode(original_model_output, skip_special_tokens=True)\n",
        "\n",
        "  instruct_model_output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "  instruct_model_summary = tokenizer.decode(instruct_model_output, skip_special_tokens=True)\n",
        "\n",
        "  peft_model_output = peft_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
        "  peft_model_summary = tokenizer.decode(peft_model_output, skip_special_tokens=True)\n",
        "\n",
        "  original_model_summaries.append(original_model_summary)\n",
        "  instruct_model_summaries.append(instruct_model_summary)\n",
        "  peft_model_summaries.append(peft_model_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TF6MuatKmFQz",
      "metadata": {
        "id": "TF6MuatKmFQz"
      },
      "source": [
        "Compute ROUGE score for this subset of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "bZmMVyDYmCDF",
      "metadata": {
        "id": "bZmMVyDYmCDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54083c17-c99e-4199-c1df-0640f7058aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.22754314914198637, 'rouge2': 0.09982267689684571, 'rougeL': 0.2116144901610018, 'rougeLsum': 0.21135161167428607}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': 0.30749690080369374, 'rouge2': 0.07752467850387067, 'rougeL': 0.23535596814257592, 'rougeLsum': 0.23560446397803886}\n",
            "PEFT MODEL:\n",
            "{'rouge1': 0.39535973659470647, 'rouge2': 0.13177960944978256, 'rougeL': 0.30808115600837016, 'rougeLsum': 0.30734436391762876}\n"
          ]
        }
      ],
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NViojzFXmRn1",
      "metadata": {
        "id": "NViojzFXmRn1"
      },
      "source": [
        "Notice, that PEFT model results are not too bad, while the training process was much easier!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C5CmTgHxmZBS",
      "metadata": {
        "id": "C5CmTgHxmZBS"
      },
      "source": [
        "Calculate the improvement of PEFT over the original model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "X2JFTooimR_t",
      "metadata": {
        "id": "X2JFTooimR_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b9d31a-65f2-4728-a7ed-14b303130615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute percentage improvement of the PEFT model over the original model:\n",
            "rouge1: 16.78%\n",
            "rouge2: 3.20%\n",
            "rougeL: 9.65%\n",
            "rougeLsum: 9.60%\n"
          ]
        }
      ],
      "source": [
        "print(\"Absolute percentage improvement of the PEFT model over the original model:\")\n",
        "\n",
        "for key in peft_model_results:\n",
        "    improvement = peft_model_results[key] - original_model_results[key]\n",
        "    print(f'{key}: {improvement*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P2yCaFcWmbWh",
      "metadata": {
        "id": "P2yCaFcWmbWh"
      },
      "source": [
        "Now calculate the improvement of PEFT over a full fine-tuned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "nNLKL9fOmc0p",
      "metadata": {
        "id": "nNLKL9fOmc0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bd0eeb-537c-4af9-af11-30145836c9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute percentage improvement of the PEFT model over the instruct model:\n",
            "rouge1: 8.79%\n",
            "rouge2: 5.43%\n",
            "rougeL: 7.27%\n",
            "rougeLsum: 7.17%\n"
          ]
        }
      ],
      "source": [
        "print(\"Absolute percentage improvement of the PEFT model over the instruct model:\")\n",
        "\n",
        "for key in peft_model_results:\n",
        "    improvement = peft_model_results[key] - instruct_model_results[key]\n",
        "    print(f'{key}: {improvement*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tpji7Bi6meQx",
      "metadata": {
        "id": "Tpji7Bi6meQx"
      },
      "source": [
        "You can see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58819c16",
      "metadata": {
        "id": "58819c16"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2ca13247ecb442997141a3e0397ad91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5c21d60eda54e7f8a73013f075c28e9",
              "IPY_MODEL_f21e9b3fa78f4c36b9a331c2dd29bd49",
              "IPY_MODEL_1881a5002a7147ae8c5af7e8b8a45209"
            ],
            "layout": "IPY_MODEL_54a154d2f58a4015b3a309acc74a90b9"
          }
        },
        "f5c21d60eda54e7f8a73013f075c28e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6e3f08b6ef4bd88b486eeecc02a2cf",
            "placeholder": "​",
            "style": "IPY_MODEL_a98d2900cda9427b83c9f428db11e6c2",
            "value": "Map: 100%"
          }
        },
        "f21e9b3fa78f4c36b9a331c2dd29bd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbe892584a2a4b67b150f06f527a8655",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a877ea25c334777a6a6af952cf14640",
            "value": 500
          }
        },
        "1881a5002a7147ae8c5af7e8b8a45209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb25c4ac19d644528a5e7383b140538a",
            "placeholder": "​",
            "style": "IPY_MODEL_06525d92e5944f7ea53568e754e6b76a",
            "value": " 500/500 [00:00&lt;00:00, 3557.60 examples/s]"
          }
        },
        "54a154d2f58a4015b3a309acc74a90b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6e3f08b6ef4bd88b486eeecc02a2cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98d2900cda9427b83c9f428db11e6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbe892584a2a4b67b150f06f527a8655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a877ea25c334777a6a6af952cf14640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb25c4ac19d644528a5e7383b140538a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06525d92e5944f7ea53568e754e6b76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc82e1aa5eb4b80803a3abc9e505b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a44358bf333c42968479c8415eb46ff9",
              "IPY_MODEL_7b99c3c3b2264a82a3004aca89d33df3",
              "IPY_MODEL_ae37ceb237f041c4aef0a876d2fdd226"
            ],
            "layout": "IPY_MODEL_a87ee9ebfd78459784060a4869af6e02"
          }
        },
        "a44358bf333c42968479c8415eb46ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_623d2701f31c4f22b36a3222373e9c80",
            "placeholder": "​",
            "style": "IPY_MODEL_e11c2c3d964d4b64b1084180ea76e83c",
            "value": "Downloading builder script: 100%"
          }
        },
        "7b99c3c3b2264a82a3004aca89d33df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea6aa9c2987d4e818f75fe817782dc3a",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d81400fd9e3d4c77aafe129a948abbcf",
            "value": 6270
          }
        },
        "ae37ceb237f041c4aef0a876d2fdd226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165719c3511b44939b6368c6944746a3",
            "placeholder": "​",
            "style": "IPY_MODEL_7326b9e8db6647c19837fa7bb69c38fc",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 515kB/s]"
          }
        },
        "a87ee9ebfd78459784060a4869af6e02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623d2701f31c4f22b36a3222373e9c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e11c2c3d964d4b64b1084180ea76e83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea6aa9c2987d4e818f75fe817782dc3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81400fd9e3d4c77aafe129a948abbcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "165719c3511b44939b6368c6944746a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7326b9e8db6647c19837fa7bb69c38fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}